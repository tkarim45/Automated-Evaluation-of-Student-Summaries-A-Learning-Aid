{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_positive(score):\n",
    "    # Assuming this converts negative scores to positive range (e.g., 0-100)\n",
    "    return max(0, min(100, (score + 1) * 50))  # Example conversion, adjust as needed\n",
    "\n",
    "def to_markdown(text):\n",
    "    # Simple markdown conversion (replace with your actual implementation if different)\n",
    "    return text.replace(\"\\n\", \"<br>\").replace(\"**\", \"<strong>\").replace(\"*\", \"<em>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taimourabdulkarim/miniforge3/envs/datality/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "from argparse import Namespace\n",
    "from transformers import DebertaModel\n",
    "\n",
    "# Define config globally\n",
    "config = Namespace(hidden_dropout_prob=0.05, hidden_size=768)\n",
    "\n",
    "class BertForSequenceRegression(nn.Module):\n",
    "    def __init__(self, pretrained_model_name=\"microsoft/deberta-base\", num_marks=2):\n",
    "        super(BertForSequenceRegression, self).__init__()\n",
    "        self.num_marks = num_marks\n",
    "        self.bert = DebertaModel.from_pretrained(pretrained_model_name)  # Fresh DeBERTa model\n",
    "        \n",
    "        self.hidden_1 = nn.Linear(2 * config.hidden_size, 2 * config.hidden_size)\n",
    "        self.notline_1 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.hidden_2 = nn.Linear(2 * config.hidden_size, config.hidden_size)\n",
    "        self.notline_2 = nn.ReLU()\n",
    "        self.dropout_2 = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.hidden = nn.Linear(config.hidden_size, 128)\n",
    "        self.regres = nn.Linear(128, num_marks)\n",
    "\n",
    "    def forward(self, input_ids, content_vector, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        output_bert = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask\n",
    "        ).last_hidden_state.mean(1)\n",
    "        h_concat = torch.cat((content_vector, output_bert), dim=1)\n",
    "        hidden_vec_1 = self.hidden_1(h_concat)\n",
    "        hidden_drop_1 = self.dropout_1(hidden_vec_1)\n",
    "        hidden_vecn_1 = self.notline_1(hidden_drop_1)\n",
    "        hidden_vec_2 = self.hidden_2(hidden_vecn_1)\n",
    "        hidden_drop_2 = self.dropout_2(hidden_vec_2)\n",
    "        hidden_vecn_2 = self.notline_2(hidden_drop_2)\n",
    "        hidden = self.hidden(hidden_vecn_2)\n",
    "        marks = self.regres(hidden)\n",
    "        return marks\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load models with fresh DeBERTa base\n",
    "model_notadhd_debarta = BertForSequenceRegression().cpu()  # Default to \"microsoft/deberta-base\"\n",
    "model_notadhd_debarta.load_state_dict(torch.load(\"Models/Not ADHD/Debarta/deb_model_3.pt\", map_location=device))\n",
    "\n",
    "model_adhd_debarta = BertForSequenceRegression().cpu()\n",
    "model_adhd_debarta.load_state_dict(torch.load(\"Models/ADHD/Debarta/adhd_deb_model_3.pt\", map_location=device))\n",
    "\n",
    "# Tokenizer and input prep\n",
    "with open(\"Models/Not ADHD/Debarta/bert_tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "summary = \"Hello My name is taimour\"\n",
    "tokens = tokenizer.tokenize(summary)\n",
    "cls_token_id = tokenizer.convert_tokens_to_ids(\"[CLS]\")\n",
    "sep_token_id = tokenizer.convert_tokens_to_ids(\"[SEP]\")\n",
    "token_index = [cls_token_id] + tokenizer.convert_tokens_to_ids(tokens) + [sep_token_id]\n",
    "\n",
    "max_length = 260\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(\"[PAD]\")\n",
    "if len(token_index) < max_length:\n",
    "    pad = [pad_token_id] * (max_length - len(token_index))\n",
    "    token_index = token_index + pad\n",
    "else:\n",
    "    token_index = token_index[:max_length]\n",
    "\n",
    "input_ids = torch.tensor([token_index], dtype=torch.long).to(device)\n",
    "attention_mask = torch.ones_like(input_ids).to(device)\n",
    "attention_mask[0, len(tokens) + 2:] = 0  # Mask padding tokens\n",
    "content_vector = torch.randn(1, config.hidden_size).to(device)\n",
    "\n",
    "# Perform prediction\n",
    "model_notadhd_debarta.eval()\n",
    "with torch.no_grad():\n",
    "    scores = model_notadhd_debarta(\n",
    "        input_ids=input_ids,\n",
    "        content_vector=content_vector,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "content_score, wording_score = scores[0, 0].item(), scores[0, 1].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_content_score = convert_to_positive(content_score)\n",
    "positive_wording_score = convert_to_positive(wording_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.8264023065567, 15.861183404922485)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_content_score, positive_wording_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
